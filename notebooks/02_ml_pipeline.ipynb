{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Machine Learning Pipeline\n",
    "\n",
    "This notebook demonstrates the Phase 2 implementation of the FinBERT-SOL project. It traces the full pipeline:\n",
    "1. Feature Engineering (`FeatureEngineer`)\n",
    "2. XGBoost Baseline Model (`XGBoostTrader`)\n",
    "3. FinBERT Sentiment Integration (`FinBERTSentiment`)\n",
    "4. Model Evaluation Framework (`ModelEvaluator`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, time\n",
    "\n",
    "# Add root directory to path to import src modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.features.feature_engineer import FeatureEngineer\n",
    "from src.models.xgboost_trader import XGBoostTrader\n",
    "from src.models.finbert_sentiment import FinBERTSentiment\n",
    "from src.evaluation.model_evaluator import ModelEvaluator"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Mock Data\n",
    "\n",
    "Since we don't have the live database connected here, we'll generate realistic mock data that mimics the `sol_ret_1h`, `bid_volume_5`, and other features used by `FeatureEngineer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_mock_data(rows=2000):\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start=\"2024-01-01\", periods=rows, freq=\"1H\")\n",
    "    \n",
    "    df = pd.DataFrame(index=dates)\n",
    "    \n",
    "    # Order flow features\n",
    "    df['best_bid'] = np.cumsum(np.random.normal(0, 0.5, rows)) + 100\n",
    "    df['best_ask'] = df['best_bid'] + np.random.uniform(0.01, 0.1, rows)\n",
    "    df['mid_price'] = (df['best_bid'] + df['best_ask']) / 2\n",
    "    df['spread'] = df['best_ask'] - df['best_bid']\n",
    "    df['spread_bps'] = (df['spread'] / df['mid_price']) * 10000\n",
    "    df['depth_imbalance_5'] = np.random.uniform(-1, 1, rows)\n",
    "    df['depth_imbalance_10'] = np.random.uniform(-1, 1, rows)\n",
    "    df['ofi_ratio'] = np.random.uniform(-1, 1, rows)\n",
    "    \n",
    "    # Volume\n",
    "    df['bid_volume_5'] = np.random.exponential(1000, rows)\n",
    "    df['ask_volume_5'] = np.random.exponential(1000, rows)\n",
    "    \n",
    "    # Technical indicators\n",
    "    df['sol_ret_1h'] = df['mid_price'].pct_change()\n",
    "    df['roc_1h'] = df['mid_price'].pct_change(periods=1)\n",
    "    df['roc_4h'] = df['mid_price'].pct_change(periods=4)\n",
    "    df['sma_diff_1h'] = np.random.normal(0, 0.01, rows)\n",
    "    df['sma_diff_4h'] = np.random.normal(0, 0.02, rows)\n",
    "    df['golden_cross'] = np.random.choice([0, 1], rows)\n",
    "    df['bb_width_1h'] = np.random.uniform(0.01, 0.05, rows)\n",
    "    df['bb_position_1h'] = np.random.uniform(-1, 1, rows)\n",
    "    \n",
    "    # Market microstructure\n",
    "    df['toxicity_bid_proxy'] = np.random.uniform(0, 1, rows)\n",
    "    df['toxicity_ask_proxy'] = np.random.uniform(0, 1, rows)\n",
    "    df['toxicity_imbalance'] = df['toxicity_bid_proxy'] - df['toxicity_ask_proxy']\n",
    "    df['bid_vwap_5'] = df['best_bid']\n",
    "    df['ask_vwap_5'] = df['best_ask']\n",
    "    df['vwap_mid_5'] = df['mid_price']\n",
    "    \n",
    "    # Cross-asset signals\n",
    "    df['btc_close'] = np.cumsum(np.random.normal(0, 50, rows)) + 40000\n",
    "    df['btc_ret_1h'] = df['btc_close'].pct_change()\n",
    "    df['rel_strength_1h'] = df['sol_ret_1h'] - df['btc_ret_1h']\n",
    "    \n",
    "    # Time features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['is_us_open'] = ((df.index.hour >= 9) & (df.index.hour < 16)).astype(int)\n",
    "    \n",
    "    return df.fillna(0)\n",
    "\n",
    "raw_data = generate_mock_data()\n",
    "print(f\"Generated {len(raw_data)} rows of market data.\")\n",
    "raw_data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "fe = FeatureEngineer()\n",
    "\n",
    "# Prepare features\n",
    "features = fe.prepare_features(raw_data)\n",
    "\n",
    "# Create labels (1h horizon)\n",
    "labels = fe.create_labels(raw_data, horizon='1h')\n",
    "\n",
    "# Align indices (feature scaling drops NAs natively)\n",
    "aligned_labels = labels.loc[features.index]\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = fe.create_train_test_split(features, aligned_labels, test_size=0.3)\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled = fe.scale_features(X_train, fit=True)\n",
    "X_test_scaled = fe.scale_features(X_test, fit=False)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "trader = XGBoostTrader()\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = trader.train_model(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "predictions, probs = trader.predict(X_test_scaled)\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "evaluator.evaluate_model_performance(\n",
    "    model_name='XGBoost Baseline',\n",
    "    y_true=y_test,\n",
    "    y_pred=predictions,\n",
    "    y_prob=probs\n",
    ")\n",
    "\n",
    "print(evaluator.generate_evaluation_report())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "trader.plot_feature_importance(top_n=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FinBERT Sentiment Filter Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sentiment_analyzer = FinBERTSentiment()\n",
    "\n",
    "# Mock some daily news for the last few days in our test set\n",
    "test_dates = list(set([str(d.date()) for d in X_test.index]))\n",
    "test_dates.sort()\n",
    "\n",
    "mock_news = {\n",
    "    test_dates[0]: [\n",
    "        {'text': 'Solana network shows strong performance with new DeFi integrations', 'time': time(8, 30)},\n",
    "        {'text': 'SOL price surges as institutional interest grows', 'time': time(9, 00)}\n",
    "    ],\n",
    "    test_dates[1]: [\n",
    "        {'text': 'Technical issues reported on Solana blockchain', 'time': time(8, 45)},\n",
    "        {'text': 'SOL experiences volatility amid market uncertainty', 'time': time(9, 15)}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Analyzing daily sentiment...\")\n",
    "daily_sentiment = sentiment_analyzer.aggregate_daily_sentiment(mock_news)\n",
    "\n",
    "for date, score in daily_sentiment.items():\n",
    "    print(f\"{date}: Sentiment Score = {score:.3f}\")\n",
    "\n",
    "sentiment_filter = sentiment_analyzer.create_sentiment_filter(daily_sentiment)\n",
    "print(\"\\nTrading Filter (Trade Allowed?):\")\n",
    "print(sentiment_filter)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backtesting with Sentiment Filtered Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "actual_returns = raw_data.loc[X_test_scaled.index, 'sol_ret_1h']\n",
    "\n",
    "# Apply the sentiment filter map to indices\n",
    "index_sentiment_filter = np.ones(len(predictions), dtype=bool)\n",
    "for i, date_idx in enumerate(X_test_scaled.index):\n",
    "    date_str = str(date_idx.date())\n",
    "    if date_str in sentiment_filter:\n",
    "        index_sentiment_filter[i] = sentiment_filter[date_str]\n",
    "\n",
    "# Evaluate Trading Performance\n",
    "metrics = evaluator.evaluate_trading_performance('XGBoost Baseline', predictions, actual_returns, index_sentiment_filter)\n",
    "\n",
    "pd.DataFrame([metrics]).T.rename(columns={0: 'Metric Value'})"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
